\center

\section{Background}

\raggedright

\subsection{Heterogeneous 1D \acrlong{MRA} model}

In our project we use the Heterogeneous 1D \acrfull{MRA} statistical model\cite{boumal2018heterogeneous} for the sake of \acrshort{cryo-EM} data abstraction. Below is the definition of the model.\\
Let $x_1,...,x_K \in \mathbb{R}^L$ be $K$ unknown normalized signals (distinct even up to shift) and let $R_s$ be the cyclic shift operator: $(R_sx)[n]=x[\langle n-s \rangle_L]$. We are given $N$ observations:

\begin{equation}
\label{eqn:MRA_obs}
y_j = R_{s_j} x_{k_j} + \varepsilon_j, \quad j=1,...,N
\end{equation}

where $s_j \sim U[0, L-1],k_j \sim U[0, K-1]$ and $\varepsilon_j \sim \mathcal{N}(0,\sigma^2I)$ is i.i.d white Gaussian noise. Our goal is to estimate the signals 
$x_1,...,x_K$ from the observations.\\
Simply speaking, \acrshort{MRA} observation is a randomly chosen signal $x_{k_j}$, shifted randomly by $s_j$ and distorted using white noise. \textbf{Fig.  \ref{fig:MRA_exmp}} shows an example of two \acrshort{MRA} observations at different noise levels.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{"figures/MRA_example".png}
  \caption{\textbf{Example of \acrshort{MRA} observations at different SNR levels.} Each column shows a shifted distinct signal at different noise levels. We can see that for low \acrshort{SNR} the task of signal estimation is quite challenging.}
  \label{fig:MRA_exmp}
\end{figure}

For the sake of data classification by the methods we suggest in our project, we define a similarity measure between observations. We use the cross-correlation, as its invariance to shift holds great value:

\begin{equation}
\label{eqn:sim_def}
(x \star y)_n \triangleq \sum_{l=\infty}^{\infty}x^*_l(y)_{n+l}
\end{equation}

The convolution theorem states that the convolution of two signals  equals to the inverse Fourier transform of the product of the Fourier transforms of each signal. Thus we can write the equation above in terms of Fourier transforms and later exploit \acrshort{FFT} in our simulations.

\begin{equation}
\label{eqn:sim_FFT}
(x \star y)_n = \mathcal{F}^{-1}\lbrace X^* \cdot Y {\rbrace}_n
\end{equation}

\clearpage

\subsection{Community detection}

As part of our suggested solution to the heterogeneity problem, we convert the \acrshort{MRA} data into a weighted graph. Each node in the graph represents a different \acrshort{MRA} observation, as was defined in \textbf{Eq.\ref{eqn:MRA_obs}}. Each edge connecting a pair of nodes has a weight that represents a similarity measure between the nodes, as was defined in \textbf{Eq. \ref{eqn:sim_FFT}}.\\

Our objective is the partition of all \acrshort{MRA} observations into $K$ non overlapping classes, where each class represents a distinct signal before it was distorted by \textbf{Eq. \ref{eqn:MRA_obs}}. Our classification process is carried out using \acrfull{CD} algorithms applied on the produced weighted graph.\\

Community, in a broad sense, is a set of nodes, which are similar to each other and dissimilar from the rest of the network. \acrfull{CD} algorithms aim to find these communities in a graph. \textbf{Fig. \ref{fig:CD}} shows the result of a \acrshort{CD} algorithm.

Finding communities in a graph carries a great value to revealing hidden patterns in data and has many use-cases, such as social behaviour prediction \cite{zachary1976} or medical diagnosis\cite{guimera2005}. Evidently, \acrlong{CD} algorithms are well studied and widely used. In our project we study a group of state-of-the-art \acrshort{CD} algorithms and apply them on \acrshort{MRA} data.\\

It is important to note that \acrshort{CD} problem is known to be \textsc{NP}-hard\cite{Fortunato_2010}, meaning that the most efficient \acrshort{CD} algorithm that partitions a graph correctly has an exponential time complexity (that is, unless \textsc{P} $=$ \textsc{NP}), making it infeasible to run on large graphs. For this reason, it is common to use \textit{heuristic} algorithms that provide approximate solution to the \acrshort{CD} problem, with the advantage of polynomial time complexity.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{"figures/CD".png}
  \caption{\textbf{Graph partition by \acrlong{CD}.} In this example for the sake of simplicity an unweighed graph is used. Note that in our project we used weighted graphs.}
  \label{fig:CD}
\end{figure}

Below is a summary of the algorithms we studied.
\begin{itemize}

\item \textit{Edge Betweenness}\cite{girvannewman2002} uses the "edge betweenness" as its optimization parameter, which is defined as the number of shortest paths that go through an edge in a graph. Following this definition, edges with higher "betweenness" can be interpreted as connections between different communities. The algorithm computes the "edge betweenness" score for each edge in the graph and iteratively removes the edges with the highest score, while computing the score for each iteration. The end product is a disconnected graph separated into separate communities.

\item \textit{Fast Greedy}\cite{Newman_2004} is based on the idea of \textit{modularity}. Let $e_{ij}$ be the fraction of edges in a graph that connect nodes in group $i$ to those in group $j$, and let 
$a_i = \sum_j e_{ij}$. Then modularity is defined as:

\begin{equation}
\label{eqn:modularity}
Q = \sum_i (e_{ii}-a_i^2)
\end{equation}

Simply speaking, modularity is the fraction of edges that fall within communities minus the expected value of the same quantity if edges fall at random. Thus, low modularity values indicate a random structure of the graph, while higher values point to deviation from randomness and more defined communities within a graph.\\
The algorithm operates by initializing a community for each node in a graph. Then it repeatedly joins nodes in pairs such that the modularity of the graph is maximized.

\item \textit{Label Propagation}\cite{Raghavan_2007} exploits the basic notion of a community as a set of similar neighbouring nodes. Each node starts with a unique label, and after each iteration, a node updates its label to the most common label of its neighbours. The algorithm stops when each node has a label that the maximum number of its neighbours have.

\item \textit{Leading Eigenvector}\cite{Newman_2006} is based on the \textit{spectral clustering}. The basic idea behind spectral clustering is to convert the graph into a \textit{similarity matrix} and find the eigenvectors that correspond to the (second) smallest eigenvalue of the matrix. These eigenvectors define the \textit{minimum cut size} of the graph (cut size is defined as the number of edges running between two groups of nodes). The second smallest eigenvalue is chosen because the smallest correspond to minimum cut size of 0, where the whole graph is a single community.\\
Let $\vec{A}$ be the  \textit{adjacency matrix}:

\begin{equation}
\label{eqn:adj_matrix}
A_{ij}=
\begin{cases}
	1 & \text{if nodes $(i,j)$ are connected}\\
	0 & otherwise
\end{cases}
\end{equation}

and $\vec{P}$, such that $P_{ij}$ is the expected number of edges between nodes $i,j$ given a random graph under the constraint that the expected \textit{degree} (number of edges connected to a node) of each node equal to the degree of each corresponding node in the input graph. \textit{Modularity matrix} is then defined as:

\begin{equation}
\label{eqn:modularity_matrix}
\vec{B}=\vec{A}-\vec{P}
\end{equation}

Leading eigenvector algorithm uses the modularity matrix as the similarity matrix and proceeds to perform a spectral clustering of the input graph.

\item \textit{Walktrap}\cite{Pons_2006} is based on the idea that short random walks on a graph tend to stay in the same densely connected area. Each node starts in its own community and a random walk process is run on each node. Based on the random walk, distances between nodes are calculated, and nodes with the smallest distances are lumped together. The process then repeats until all nodes are in the same community. At each step modularity (\textbf{Eq. \eqref{eqn:modularity}}) of the partition is computed, and in the end the partition with the maximum modularity is chosen.

\item \textit{Infomap}\cite{Rosvall_2009} uses \textit{Huffman code} as a way to compress the information about a path of a random walk in a graph.
Each node in a graph is given a unique Id. A random walk that explores the entire graph is initialized, note that random walk tends to stay longer in a densely connected areas. This allows to combine nodes Ids into Huffman codes, that will ultimately label the communities in a graph. The algorithm then optimizes the number of Huffman codes such that the encoded information about the path of the random walk in a graph is maximally compressed.

\item \textit{Louvain}\cite{Blondel_2008} maximizes modularity (\textbf{Eq. \eqref{eqn:modularity}}) in a hierarchical fashion. In the first phase of the algorithm, each node starts in its own community. Each node is then placed in a community with its neighbour such that a maximum modularity score is obtained for the graph. In the second phase a new graph is created, whose nodes are now the communities found during the first phase, such that the weight of the edges between these nodes equal to the edge density between the communities from the first phase. After the second phase is complete, the new graph is passed through the first phase again. The process is repeated until no modularity gain is obtained.

\item \textit{Leiden}\cite{Traag_2019} is an improvement of the Louvain algorithm. Louvain algorithm, in some cases, has proven to produce bad partitions in the form of disconnected communities (a community that contains disconnected nodes). Leiden algorithm introduces a refinement stage in the first phase (see Louvain algorithm). At the second phase, the refined partition is aggregated and initially partitioned based on the unrefined partition. By creating the aggregated graph based on the refined partition, Leiden algorithm has more room for identifying high-quality partitions.\\
Simply speaking, when a community is aggregated to a node at the second phase in Louvain algorithm, no further changes inside the community are possible. Leiden, on the other hand, gives more flexibility by refining the graph in the first phase.\\
Leiden also uses a faster method for implementing the nodes transitions between communities in the first phase.

\item \textit{SpinGlass}\cite{Reichardt_2006} adopts ideas from the field of statistical mechanics for the clustering of the graph, and is based on the Potts model \cite{RevModPhys.54.235}. The algorithm regards to the graph as a system with at most of $k$ \textit{spin states} (which are equivalent to the number of communities we wish to find), and tries to minimize the collective \textit{energy} of the system by arranging its \textit{particles} (which are equivalent to the nodes of the graph) into said spin states. \\ The energy of the system is quantified by the \textit{Hamiltonian} function, which is defined for a group of spin states ${\sigma_{i}}$ as:

\begin{align*}
\label{eqn:hamiltonian}
&-\sum_{i\neq j} a_{ij}\cdot \underbrace{A_{ij} \delta{\sigma_i,\sigma_j)}}_{Internal Edges}
+\sum_{i\neq j} b_{ij}\cdot \underbrace{(1-A_{ij}) \delta{\sigma_i,\sigma_j}}_{Non-existing Internal Edges}\\&
+\sum_{i\neq j} c_{ij}\cdot \underbrace{A_{ij} (1-\delta{\sigma_i,\sigma_j)}}_{External Edges} 
\sum{i\neq j} d_{ij}\cdot \underbrace{(1-A_{ij}) (1-\delta{\sigma_i,\sigma_j)}}_{Non-existing External Edges}
\end{align*}

where $\vec{A}$ is the adjacency matrix (\textbf{Eq. \ref{eqn:adj_matrix}}) of the graph, $a_{ij}$,$b_{ij}$,$c_{ij}$ and $d_{ij}$ are hyper parameters which affect the contribution of each factor to the hamiltonian and can be changed and $\delta(a,b)$ symbolizes Kronecker delta.
In order to minimize the hamiltonian, the algorithm also uses simulated annealing\cite{10.5555/800044.801546}.

\end{itemize}

\textbf{Table \ref{tab:complex}} summarizes time complexities of each algorithm.

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Algorithm} & \textbf{Complexity} & \textbf{Notes}\\
		\hline
		Edge Betweenness & $O(|E||V|(|E|+|V|))$ & \\
		Fast Greedy & $O(|V|(|E|+|V|))$ & \\
		Label Propagation & $O(|E|)$ & \\
		Leading Eigenvector & $O(|V|(|E|+|V|))$ & \\
		Walktrap & $O(D|E||V|^2)$ & $D$ the size of the dendrogram \\
		Infomap & $O(t^2|E|)$ & $t$ maximal number of iterations\\
		Louvain & $O(|E|)$ & \\
		Leiden & $O(|E|)$ & \begin{tabular}{@{}c@{}}
Considered to be superior to Louvain,\\ i.e its constant factor is smaller		
\end{tabular}\\
		SpinGlass & $O(|V|^{3.2})$ & For sparse graphs\\
		\hline
		\end{tabular}
		\caption{\textbf{\acrshort{CD} algorithms time complexities.} $|E|$ represents the total number of edges in the graph and $|V|$ the total number of nodes (vertices).}
		\label{tab:complex}
	\end{center}
\end{table}


\subsection{Performance evaluation}
In order to compare performance between the different \acrshort{CD} algorithms, several criteria of performance evaluation need to be defined:
\begin{itemize}

\item \textbf{The (asymptotic) running time complexity of each algorithm}, or for the sake of simplicity, 'how many steps the algorithm should do on given inputs until it converges, depending on the input's size'. For this criterion, we used the traditional big-O notation. 

\item \textbf{A scoring function to measure the accuracy of the partition an algorithm yielded}, by measuring the degree of similarity between the partition produced by an algorithm and the correct partition of the graph, i.e. the \textit{ground truth}.\\
As was mentioned previously, \acrshort{CD} algorithms are heuristic for solving the \textsc{NP}-hard graph partition problem, therefore cannot always cluster the graph perfectly, assuming that \textsc{P} $\neq$ \textsc{NP}. Thus a scoring function can be introduced to provide us a way to quantify the quality of each partition. In our project we chose the \acrfull{NMI} as the accuracy score function, as it is considered a good method to measure \acrshort{CD} algorithms partition accuracy\cite{Danon_2005}.\\
We start by introducing the \acrfull{MI} scoring function, which incorporates ideas from the information theory. 
Let $G=(V,E)$ be an unweighed graph and let $P$ be a partition of the graph to $M$ communities $C_1,C_2,â€¦,C_M$. \textit{Shannon entropy} is then defined as
\begin{equation}
\label{eqn:Shannon}
H(P)= -\sum_{i} \frac{|C_i|}{V}log(\frac{|C_i|}{V})
\end{equation}
where $|C_i|$ is the number of nodes in $C_i$. It is also convenient to define the \textit{conditional Shannon entropy}
\begin{equation}
\label{eqn:cond_Shannon}
H(P_{1}|P_{2})= -\sum_{i,j} \frac{|C_{i}\cap C_{j}|}{V}log(\frac{|C_{i}\cap C_{j}|}{V})
\end{equation}
where $P_1$,$P_1$ different partitions of $G$, and $|C_i \cap C_j|$ number of nodes that belong to both $C_i$,$C_j$. \acrshort{MI} is then defined as:
\begin{equation}
\label{eqn:MI}
MI(P_{1},P_{2}) = H(P_{1})-H(P_{1}|P_{2})
\end{equation}
One can note that the range of the \acrshort{MI} quality function is $[0,\infty)$ making it impractical for comparing many cases. Thus it is useful to define the \acrshort{NMI} function:
\begin{equation}
\label{eqn:NMI}
NMI(P_{1},P_{2})= \frac{2MI(P_{1},P_{2})}{H(P_{1})+H(P_{2})}
\end{equation} 
Note that the range of the \acrshort{NMI} quality function is $[0,1]$, where \acrshort{NMI}=1 if the partitions are identical and \acrshort{NMI}=0 if the partitions are completely different.\\
A very useful property of the \acrshort{NMI} is its independency from the number of communities in each partition, providing us a convenient tool to compare two partitions with different number of communities.

\end{itemize}
A few important remarks about performance measurement should be noted:
\begin{enumerate}

\item \textbf{There is no absolute performance metric}, meaning
an algorithm's performance may receive a 'good' score by some of the criteria but a 'bad' score by other criteria.

\item \textbf{An algorithm performance may depend on the input itself}, that is, an algorithm may perform well on some inputs, but badly on others. In order to reduce this dependability, we should analyze an algorithm's performance by considering the \textit{expectation} of its performance according to each criterion on several random inputs.

\item \textbf{Comparison between algorithms should be done on the same inputs exactly}. In other words, it is not enough to run each algorithm on several random inputs separately, and then to compare the results.

\end{enumerate}

\vspace{10pt}

\subsection{Modified K-means algorithm}
In order to examine if \acrshort{CD} is beneficial compared to other clustering methods, we compare the \acrshort{CD} algorithms performance against a different method of clustering. We present a modified K-means algorithm that takes into account the fact that the \acrshort{MRA} data is shifted.
We start by introducing the traditional KMeans algorithm\cite{10.5555/46712}, a well known clustering algorithm used widely for pattern recognition applications.
Let $X=x_1,...,x_n \in \mathbb{R}^L$ be the set of $L$-dimensional points to be clustered into $K$ clusters, $C=c_1,...,c_K$. Each cluster is represented by a centroid $\mu_k$, $k=1,...,K$, which is the mean of all points in the cluster. K-means finds a partition of $X$ into $K$ clusters such that the squared error of the distances between the centroids of the clusters and the points in the corresponding clusters is minimized. Therefore, the goal of K-means is to minimize the function in \textbf{Eq. \ref{eqn:kmeans_error}}.

\begin{equation}
\label{eqn:kmeans_error}
J(C)=\sum_{k=1}^K{\sum_{x_i \in c_k}{||x_i-\mu_k||^2}}
\end{equation} 

Minimizing the given function is an \textsc{NP}-hard problem \cite{Drineas_1999}, thus can only converge to a local minimum.\\
The algorithm's operation can be summarized in the following steps:
\begin{enumerate}
\item Select an initial partition of the data set into $K$ partitions ($K$ is a parameter of the algorithm). In our project we used the Kmeans++ implementation\cite{10.5555/1283383.1283494}:
	\begin{enumerate}
	\item Choose a random point from $X$ to be the first centroid.
	\item Choose the next centroid $\mu_k$, selecting $\mu_k=x' \in X$ with probability $\frac{D(x')^2}{\sum_{x \in X}{D(x)^2}}$, where $D$ is distance from the centroid.
	\item Repeat steps (b) until $K$ centroids were chosen.
	\end{enumerate}
\item Assign each point in the data set to its closest centroid, i.e assign $x_i$ to cluster $c_k$ such that $||x_i-\mu_k||^2$ is minimal for $k=1,...,K$.
\item Recalculate the centroids of the new clusters by computing the mean of each new cluster.
\item Repeat steps (2) and (3) until cluster membership stabilizes.
\end{enumerate}
Our modified algorithm introduces two adjustments to the algorithm presented above:
\begin{enumerate}
\item The distance metric used is the Euclidean distance between pairs of centroids and data points shifted, such that the cross correlation between each data point and centroid is maximal. Simply, data points were first shifted, then the distance was calculated.
\item Centroids were calculated using \textit{template matching}, i.e first sample of each cluster was chosen as the reference point (template) and every other point in the cluster was shifted such that the cross-correlation between each point and the first point is maximal. After the shifting mean is calculated.
\end{enumerate}
For a fixed maximal number of iterations $t$ the time complexity of K-means is $O(ndKt)$ where $n$ is the number of data points, $d$ the dimension of each point and $K$ number of specified clusters.